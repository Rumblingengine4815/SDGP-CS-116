import pandas as pd
import numpy as np
import sys
import json
import re
import torch
import fitz # PyMuPDF for Resume Parsing
from sentence_transformers import SentenceTransformer, util
from pathlib import Path
import traceback
from typing import List, Dict, Any, Optional
from pymongo import MongoClient
from dotenv import load_dotenv
import os

try:
    from PIL import Image
    import pytesseract
    HAS_OCR = True
except ImportError:
    HAS_OCR = False

# Add utils to path for trend analyzer
try:
    from .utils.market_trend_analyzer import MarketTrendAnalyzer
except (ImportError, ValueError):
    try:
        from utils.market_trend_analyzer import MarketTrendAnalyzer
    except ImportError:
        import sys
        sys.path.append(str(Path(__file__).parent / "utils"))
        from market_trend_analyzer import MarketTrendAnalyzer


class RecommendationEngine:
    def __init__(self, jobs_path=None, courses_path=None, esco_dir=None, models_dir=None, force_refresh=False, show_progress=True, from_mongo=False):
        # Global Root Detection (Relative to core/)
        self.ml_root = Path(__file__).resolve().parent.parent
        self.show_progress = show_progress
        
        #State Initialization 
        self.jobs_df = pd.DataFrame()
        self.courses_df = pd.DataFrame()
        self.academic_df = pd.DataFrame()
        self.career_progressions_df = pd.DataFrame()
        self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
        self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
        self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
        self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])
        self.mentors_data = []
        self.salary_data = {}
        self.pricing_config = {}
        self.assessment_config = {}
        self.assessment_questions = {}
        self.market_skills = []
        self._trend_cache = {}
        
        #  Heavy Model Loading
        try:
            self.model = SentenceTransformer("all-MiniLM-L6-v2")
        except Exception as e:
            if self.show_progress: print(f"CRITICAL: Failed to load Transformer model: {e}")
            raise
        
        #  Data Loading Logic
        try:
            if from_mongo:
                self.load_from_mongo()
            else:
                # Default local paths if none provided
                jobs_path = jobs_path or str(self.ml_root / "data/processed/all_jobs_master.csv")
                courses_path = courses_path or str(self.ml_root / "data/processed/all_courses_master.csv")
                esco_dir = esco_dir or str(self.ml_root / "data/raw/esco")
                self._load_from_local(jobs_path, courses_path, esco_dir)
        except Exception as e:
            if self.show_progress: 
                print(f"Warning: Primary data loading interrupted: {e}")

        #  Common Post-Load Setup
        # Use ML root for models if not specified
        models_dir = models_dir or str(self.ml_root / "models")
        self._initialize_common(models_dir, force_refresh, courses_path)

    @classmethod
    def from_mongo(cls):
        """Factory method to initialize engine from MongoDB cloud data."""
        return cls(from_mongo=True)

    def load_from_mongo(self):
        """Fetches all primary datasets and configs from MongoDB Atlas."""
        if self.show_progress: print(" FETCHING DATA FROM MONGODB ATLAS")
        try:
            # Check for .env in current root
            env_path = self.ml_root / ".env"
            load_dotenv(dotenv_path=env_path if env_path.exists() else None)
            
            client = MongoClient(os.getenv("MONGO_URI"))
            db = client[os.getenv("DATABASE_NAME", "pathfinder_plus")]
            
            #  Load Jobs
            self.jobs_df = pd.DataFrame(list(db.jobs.find({}, {'_id': 0})))
            syn_jobs = pd.DataFrame(list(db.jobs_synthetic.find({}, {'_id': 0})))
            if not syn_jobs.empty:
                self.jobs_df = pd.concat([self.jobs_df, syn_jobs], ignore_index=True)
            
            # Load Courses
            self.courses_df = pd.DataFrame(list(db.courses.find({}, {'_id': 0})))
            self.academic_df = pd.DataFrame(list(db.courses_academic.find({}, {'_id': 0})))

            # Standardization Helper for Courses
            for df in [self.courses_df, self.academic_df]:
                if not df.empty:
                    if "course_title" not in df.columns and "course_name" in df.columns:
                        df.rename(columns={"course_name": "course_title"}, inplace=True)
                    if "provider" not in df.columns and "institute" in df.columns:
                        df.rename(columns={"institute": "provider"}, inplace=True)
            
            if self.show_progress:
                print(f"DEBUG: courses_df columns: {self.courses_df.columns.tolist()}")
                print(f"DEBUG: academic_df columns: {self.academic_df.columns.tolist()}")
            
            #  Load Mentors
            self.mentors_data = list(db.mentors.find({}, {'_id': 0}))
            
            #  Load Progressions
            self.career_progressions_df = pd.DataFrame(list(db.career_paths.find({}, {'_id': 0})))
            
            # 5 Load Salary Data
            salary_list = list(db.salary_data.find({}, {'_id': 0}))
            self.salary_data = {}
            for item in salary_list:
                title = item.get('job_title', item.get('title', 'Unknown'))
                min_s = item.get('salary_min', 0)
                max_s = item.get('salary_max', 0)
                self.salary_data[str(title).lower()] = f"{min_s} - {max_s} LKR"
            
            #  Load Configs from app_configs collection
            configs = list(db.app_configs.find({}, {'_id': 0}))
            config_dict = {cfg['config_key']: cfg['data'] for cfg in configs}
            self.pricing_config = config_dict.get('pricing_estimates', {})
            self.assessment_config = config_dict.get('scoring_config', {})
            self.assessment_questions = config_dict.get('assessment_questions', {})

            #  Load ESCO dataframes from Cloud
            self.esco_occ = pd.DataFrame(list(db.esco_occupations.find({}, {'_id': 0})))
            self.esco_skills = pd.DataFrame(list(db.esco_skills.find({}, {'_id': 0})))
            self.occ_skill_rel = pd.DataFrame(list(db.esco_relations.find({}, {'_id': 0})))
            self.broader_occ = pd.DataFrame(list(db.esco_broader.find({}, {'_id': 0})))

            # Fallback to empty DFs if collections missing
            if self.esco_occ.empty:
                self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            if self.esco_skills.empty:
                self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            if self.occ_skill_rel.empty:
                self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
            if self.broader_occ.empty:
                self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])

            # Ensure columns exist even if DF is empty
            for df_name, df_obj in [("courses_df", self.courses_df), ("academic_df", self.academic_df)]:
                 if df_obj.empty:
                     print(f"WARNING: {df_name} is EMPTY after cloud load.")
                 else:
                     print(f"INFO: {df_name} loaded with {len(df_obj)} rows. Columns: {df_obj.columns.tolist()}")

            if self.show_progress: print(f"Cloud Load Complete: {len(self.jobs_df)} jobs, {len(self.courses_df)} courses.")
            
        except Exception as e:
            if self.show_progress: print(f"Cloud Load Failed: {e}. Falling back to empty data.")
            import traceback
            traceback.print_exc()
            self.jobs_df = pd.DataFrame()
            self.courses_df = pd.DataFrame(columns=["course_title", "provider", "category", "description"])
            self.academic_df = pd.DataFrame(columns=["course_title", "provider", "category", "description"])
            self.mentors_data = []
            self.career_progressions_df = pd.DataFrame()
            self.salary_data = {}
            self.pricing_config = {}
            self.assessment_config = {}
            self.assessment_questions = {}
            self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
            self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])


    def _load_from_local(self, jobs_path, courses_path, esco_dir):
        """Legacy local CSV loading logic."""
        if not jobs_path or not courses_path or not esco_dir:
            if self.show_progress: print("Warning: Local paths missing. Initialize with from_mongo=True or provide paths.")
            self.jobs_df = pd.DataFrame()
            self.courses_df = pd.DataFrame()
            self.academic_df = pd.DataFrame()
            self.mentors_data = []
            self.career_progressions_df = pd.DataFrame()
            self.salary_data = {}
            self.pricing_config = {}
            self.assessment_config = {}
            self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
            self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])
            return

        # Load pricing config
        self.pricing_config = {}
        config_path = Path(jobs_path).parent.parent / "config" / "pricing_estimates.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                self.pricing_config = json.load(f)
        else:
             if self.show_progress: print(f"Warning: Pricing config not found at {config_path}")

        # Load datasets
        self.jobs_df = pd.read_csv(jobs_path)
        self.courses_df = pd.read_csv(courses_path)
        
        # Standardize main courses_df
        if not self.courses_df.empty:
            if "course_title" not in self.courses_df.columns and "course_name" in self.courses_df.columns:
                self.courses_df.rename(columns={"course_name": "course_title"}, inplace=True)
            if "provider" not in self.courses_df.columns and "institute" in self.courses_df.columns:
                self.courses_df.rename(columns={"institute": "provider"}, inplace=True)

        #  Load Academic Courses separately
        self.academic_df = pd.DataFrame()
        academic_path = Path(courses_path).parent / "academic_courses_master.csv"
        if academic_path.exists():
            if self.show_progress: print(f"Loading academic courses ({academic_path.name})...")
            self.academic_df = pd.read_csv(academic_path)
            # Ensure naming consistency
            if "course_title" not in self.academic_df.columns and "course_name" in self.academic_df.columns:
                self.academic_df.rename(columns={"course_name": "course_title"}, inplace=True)
            if "provider" not in self.academic_df.columns and "institute" in self.academic_df.columns:
                self.academic_df.rename(columns={"institute": "provider"}, inplace=True)
        else:
            if self.show_progress: print(f"Warning: {academic_path} not found.")

        #  Load Synthetic Jobs if available (Demo Enhancement)
        syn_jobs_path = Path(jobs_path).parent / "synthetic_jobs.csv"
        if syn_jobs_path.exists():
            if self.show_progress: print(f"Merging synthetic jobs ({syn_jobs_path.name})...")
            syn_df = pd.read_csv(syn_jobs_path)
            self.jobs_df = pd.concat([self.jobs_df, syn_df], ignore_index=True)

        #  Load Synthetic Mentors
        self.mentors_data = []
        mentors_path = Path(jobs_path).parent / "mentors.json"
        if mentors_path.exists():
            try:
                with open(mentors_path, 'r') as f:
                    self.mentors_data = json.load(f)
                if self.show_progress: print(f"Loaded {len(self.mentors_data)} mentors.")
            except Exception:
                pass

        #  Load Career Progressions
        prog_path = Path(jobs_path).parent / "career_progressions.csv"
        if prog_path.exists():
            self.career_progressions_df = pd.read_csv(prog_path)
            if self.show_progress: print(f"Loaded {len(self.career_progressions_df)} career progression paths.")
        else:
            self.career_progressions_df = pd.DataFrame()
            if self.show_progress: print("Warning: career_progressions.csv not found.")

        # Load market context advice
        advice_path = Path(esco_dir).parent / "market_context" / "sl_sector_advice.csv"
        if advice_path.exists():
            self.market_advice_df = pd.read_csv(advice_path)
        else:
            self.market_advice_df = None

        # LOAD ESCO DATA 
        try:
            esco_path = Path(esco_dir)
            if (esco_path / "occupations_en.csv").exists():
                self.esco_occ = pd.read_csv(esco_path / "occupations_en.csv")
                self.esco_skills = pd.read_csv(esco_path / "skills_en.csv")
                self.occ_skill_rel = pd.read_csv(esco_path / "occupationSkillRelations_en.csv")
            else:
                if self.show_progress: print("WARNING: ESCO files not found. Using empty dataframes.")
                self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
                self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
                self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
        except Exception as e:
            if self.show_progress: print(f"ERROR loading ESCO: {e}. Falling back to empty data.")
            self.esco_occ = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.esco_skills = pd.DataFrame(columns=["preferredLabel", "conceptUri"])
            self.occ_skill_rel = pd.DataFrame(columns=["occupationUri", "skillUri", "relationType"])
        # load broader occupations for progression (With Fallbacks)
        try:
            if (esco_path / "broaderRelationsOccPillar_en.csv").exists():
                self.broader_occ = pd.read_csv(esco_path / "broaderRelationsOccPillar_en.csv")
            else:
                self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])
        except Exception:
            self.broader_occ = pd.DataFrame(columns=["conceptUri", "broaderUri"])

 
    def _initialize_common(self, models_dir, force_refresh, courses_path):
        """Standard setup logic for both local and cloud modes."""
        #  Models Path
        if models_dir is None:
            models_path = Path(__file__).parent.parent / "models"
        else:
            models_path = Path(models_dir)
        models_path.mkdir(parents=True, exist_ok=True)

        #  Extract Market Skills (Critical for matching logic)
        self.market_skills = []
        if hasattr(self, 'jobs_df') and not self.jobs_df.empty and 'extracted_skills' in self.jobs_df.columns:
            all_skills = []
            for s in self.jobs_df['extracted_skills'].dropna():
                if isinstance(s, str):
                    all_skills.extend([sk.strip().lower() for sk in s.split(',')])
            self.market_skills = list(set(all_skills))
        
        #  Initialize Trend Analyzer
        try:
             # In cloud mode, courses_path might be None, but analyzer needs jobs data
             # We use jobs_df directly from the engine
            self.trend_analyzer = MarketTrendAnalyzer(self.jobs_df)
        except Exception as e:
            if self.show_progress: print(f"Warning: Trend Analyzer failed: {e}")
            self.trend_analyzer = None

        self._trend_cache = {}

        #  Load or Build Embeddings
        self._load_or_build_embeddings(models_path, force_refresh, courses_path)

    def _load_or_build_embeddings(self, models_path, force_refresh, courses_path):
        """Logic moved from legacy init into a dedicated method."""
        # Ensure courses_path is string-safe for filename generation
        course_filename = Path(courses_path).stem if courses_path else "cloud_courses"
        
        if self.show_progress:
            print(f"DEBUG EMBED: jobs_df columns: {self.jobs_df.columns.tolist() if not self.jobs_df.empty else 'EMPTY'}")
            print(f"DEBUG EMBED: courses_df columns: {self.courses_df.columns.tolist() if not self.courses_df.empty else 'EMPTY'}")
     
        # Load assessment config
        config_path = Path(__file__).parent.parent / "data" / "raw" / "assessment" / "scoring_config.json"
        if config_path.exists():
            with open(config_path, 'r') as f:
                self.assessment_config = json.load(f)
        else:
            self.assessment_config = {}

        # load esco embeddings
        esco_emb_file = models_path / "esco_occ_embeddings.pt"
        rebuild_esco = True
        
        if esco_emb_file.exists() and not force_refresh:
            if self.show_progress: print(f"Loading pre-computed ESCO embeddings from {esco_emb_file}")
            try:
                loaded_esco = torch.load(esco_emb_file)
                if len(loaded_esco) == len(self.esco_occ):
                    self.esco_occ_embs = loaded_esco
                    rebuild_esco = False
                else:
                    print(f"WARNING: ESCO Embedding size ({len(loaded_esco)}) != Dataframe size ({len(self.esco_occ)}). Rebuilding...")
            except Exception as e:
                print(f"Error loading ESCO embeddings: {e}. Rebuilding...")

        if rebuild_esco:
            if self.show_progress: print("Encoding ESCO occupations...")
            self.esco_occ_embs = self.model.encode(
                self.esco_occ["preferredLabel"].tolist(),
                convert_to_tensor=True,
                show_progress_bar=self.show_progress,
            )
            torch.save(self.esco_occ_embs, esco_emb_file)

        # load Embeddings for (Professional)
        course_emb_file = models_path / f"course_embeddings_{course_filename}.pt"
        
        rebuild_courses = True
        if course_emb_file.exists() and not force_refresh:
            if self.show_progress: print(f"Loading pre-computed course embeddings from {course_emb_file}")
            try:
                loaded_embs = torch.load(course_emb_file)
                if len(loaded_embs) == len(self.courses_df):
                    self.course_embs = loaded_embs
                    rebuild_courses = False
                else:
                    print(f"WARNING: Embedding size ({len(loaded_embs)}) != Dataframe size ({len(self.courses_df)}). Rebuilding...")
            except Exception:
                pass
        
        if rebuild_courses:
            if self.show_progress: print(f"Encoding professional courses for {course_filename}...")
            course_texts = (
                self.courses_df["course_title"].fillna("")
                + " "
                + self.courses_df["category"].fillna("")
                + " "
                + self.courses_df["description"].fillna("")
            ).tolist()

            self.course_embs = self.model.encode(
                course_texts, convert_to_tensor=True, show_progress_bar=self.show_progress
            )
            torch.save(self.course_embs, course_emb_file)

        # laoding acadmeic embeddings
        academic_emb_file = models_path / "academic_embeddings.pt"
        rebuild_academic = True
        
        if not self.academic_df.empty:
            if academic_emb_file.exists() and not force_refresh:
                if self.show_progress: print(f"Loading pre-computed academic embeddings from {academic_emb_file}")
                try:
                    loaded_acad = torch.load(academic_emb_file)
                    if len(loaded_acad) == len(self.academic_df):
                        self.academic_embs = loaded_acad
                        rebuild_academic = False
                    else:
                        print(f"WARNING: Academic Embedding size ({len(loaded_acad)}) != Dataframe size ({len(self.academic_df)}). Rebuilding...")
                except Exception:
                    pass

            if rebuild_academic:
                if self.show_progress: print("Encoding academic degree programs...")
                acad_texts = (
                    self.academic_df["course_title"].fillna("")
                    + " "
                    + self.academic_df["category"].fillna("")
                    + " "
                    + self.academic_df["description"].fillna("")
                ).tolist()
                
                self.academic_embs = self.model.encode(
                    acad_texts, convert_to_tensor=True, show_progress_bar=self.show_progress
                )
                torch.save(self.academic_embs, academic_emb_file)
        else:
            self.academic_embs = None

        # Job emddding load
        job_emb_file = models_path / "job_embeddings.pt"
        rebuild_jobs = True
        
        if not self.jobs_df.empty and "title" in self.jobs_df.columns:
            # We only use titles for the moment for semantic search speed
            self.job_titles_list = self.jobs_df["title"].fillna("").tolist()
            
            if job_emb_file.exists() and not force_refresh:
                if self.show_progress: print(f"Loading pre-computed Job embeddings from {job_emb_file}")
                try:
                    loaded_jobs = torch.load(job_emb_file)
                    if len(loaded_jobs) == len(self.job_titles_list):
                        self.job_embs = loaded_jobs
                        rebuild_jobs = False
                    else:
                        print(f"WARNING: Job Embedding size ({len(loaded_jobs)}) != Dataframe size ({len(self.job_titles_list)}). Rebuilding...")
                except Exception as e:
                    print(f"Error loading Job embeddings: {e}. Rebuilding...")

            if rebuild_jobs:
                if self.show_progress: print(f"Encoding {len(self.job_titles_list)} jobs...")
                self.job_embs = self.model.encode(
                    self.job_titles_list,
                    convert_to_tensor=True,
                    show_progress_bar=self.show_progress
                )
                torch.save(self.job_embs, job_emb_file)
        else:
            self.job_embs = None
            self.job_titles_list = []

    def get_salary_for_role(self, role_title):
        """Retrieves salary range for a given role title (fuzzy match)"""
        role_key = str(role_title).strip().lower()
        
        #  Exact Match
        if role_key in self.salary_data:
            return self.salary_data[role_key]
            
        #  Fuzzy Match (Contained)
        for key, val in self.salary_data.items():
            if key in role_key or role_key in key:
                return val
                
        return "Data Not Available"

    def process_comprehensive_assessment(self, answers: Dict[str, Any]):
        """
        Processes the 18-point comprehensive assessment into a Feature Vector for ML.
        """
        vector = {}
        #  Normalize strings to handle hyphen/en-dash variations (e.g. 1-3 vs 1–3)
        norm_answers = {k: str(v).replace('–', '-').replace('—', '-') if isinstance(v, str) else v for k, v in answers.items()}
        
        #  Base Levels
        status_map = self.assessment_config.get("mapping", {}).get("status", {
            "O/L Student": 0, "A/L Student": 0, "Undergraduate": 1, "Graduate": 2,
            "Working Professional": 3, "Career Transitioning": 3
        })
        status = norm_answers.get("status", "Undergraduate")
        vector["status_level"] = status_map.get(status, 1)
        
        experience_map = self.assessment_config.get("mapping", {}).get("experience", {
            "None": 0, "< 1 year": 0.5, "1-3 years": 2, "3-5 years": 4, "5+ years": 6
        })
        experience = norm_answers.get("total_experience", "None")
        vector["experience_years"] = experience_map.get(experience, 0)
        
        #  Responsibility Band (0-4)
        resp_level = norm_answers.get("responsibility_level", "Followed instructions")
        band_map = {
            "Followed instructions": 0,
            "Completed independent tasks": 1,
            "Planned tasks": 2,
            "Supervised others": 3,
            "Managed outcomes / budgets": 4
        }
        vector["responsibility_band"] = band_map.get(resp_level, 0)
        
        # Behavioral Scores (0-3)
        logic = self.assessment_config.get("assessment_logic", {})
        vector["problem_solving_score"] = logic.get("problem_solving", {}).get(norm_answers.get("q7"), 0)
        vector["decision_making_score"] = logic.get("decision_making", {}).get(norm_answers.get("q8"), 0)
        vector["leadership_score"] = logic.get("team_role", {}).get(norm_answers.get("q9"), 0)
        vector["adaptability_score"] = logic.get("adaptability", {}).get(norm_answers.get("q10"), 0)
        vector["initiative_score"] = logic.get("efficiency", {}).get(norm_answers.get("q11"), 0)
        vector["conflict_score"] = logic.get("conflict", {}).get(norm_answers.get("q12"), 0)
        
        #  Semantic Intent (Open Prompts)
        prompts = [
            norm_answers.get("q13", ""), # Proud project
            norm_answers.get("q14", ""), # Environment
            norm_answers.get("q15", ""), # Success outcome
            norm_answers.get("q16", "")  # Obstacles
        ]
        full_text = " ".join([p for p in prompts if p])
        if full_text:
            vector["intent_embedding"] = self.model.encode(full_text).tolist()
            # Extract keywords for skill matching
            extracted = [s for s in self.market_skills if s.lower() in full_text.lower()]
            vector["extracted_intent_skills"] = extracted
        else:
            vector["intent_embedding"] = []
            vector["extracted_intent_skills"] = []

        # Constraints
        vector["budget_category"] = norm_answers.get("budget_range", "None")
        vector["time_commitment"] = norm_answers.get("weekly_time", "None")
        vector["education_preference"] = norm_answers.get("education_type", "None")
        
        return vector

    def _should_recommend_internships(self, assessment_vector: Dict[str, Any]) -> bool:
        """Strict check to ensure professionals don't get internship paths//"""
        exp = assessment_vector.get("experience_years", 0)
        status = assessment_vector.get("status_level", 0)
        # Professionals (> 0 exp or status level 3) should usually bypass internships
        return exp == 0 and status <= 1

    def get_recommendations_from_assessment(self, assessment_vector: Dict[str, Any], target_job: str):
        """
        Entry point for the new Assessment-driven recommendation logic.
        Returns a complete 'Dashboard Bundle' for UI/Chatbot display.
        """
        # 1. Determine segment and user_level
        status_level = assessment_vector.get("status_level", 0)
        segment = "Student" if status_level <= 1 else "Professional"
        user_skills = assessment_vector.get("extracted_intent_skills", [])
        
        # Mapping user levels for better course query results
        user_level = "Entry"
        if status_level == 0: user_level = "O/L or School Level"
        elif status_level == 1: user_level = "A/L or Undergraduate"
        elif status_level == 2: user_level = "Professional"
        elif status_level >= 3: user_level = "Senior / Expert"

        # 2. Parse Constraints
        budget_str = str(assessment_vector.get("budget_category", "")).replace("–", "-")
        budget_map = {"< 50k": 50000, "50k-200k": 200000, "200k-500k": 500000, "500k+": 2000000}
        max_budget = budget_map.get(budget_str)
        
        # 3. Fetch Components
        # Use specific preference mapping
        pref = assessment_vector.get("education_preference")
        if not pref or pref == "None":
            if status_level == 0: pref = "Diploma"
            elif status_level == 1: pref = "BSc"
            elif status_level >= 2: pref = "MSc"

        courses = self.recommend_courses(
            user_skills=user_skills,
            target_job=target_job,
            user_level=user_level,
            segment=segment,
            preference=pref,
            max_budget=max_budget,
            top_n=5,
            assessment_vector=assessment_vector  # Pass full context
        )
        
        # Ensure we return a structured bundle
        return courses

    def recommend_jobs(self, user_skills, target_role, top_n=5):
        """Matches users to real-time job openings from the database."""
        if self.jobs_df.empty or not hasattr(self, 'job_embs') or self.job_embs is None:
             return []
        
        try:
            # . Build Query text
            query = f"{target_role} " + " ".join(user_skills[:5])
            query_emb = self.model.encode(query, convert_to_tensor=True)
            
            # Semantic Search
            hits = util.semantic_search(query_emb, self.job_embs, top_k=top_n*2)[0]
            
            results = []
            for hit in hits:
                idx = hit['corpus_id']
                if idx >= len(self.jobs_df): continue
                job = self.jobs_df.iloc[idx]
                
                results.append({
                    "job_title": job["title"],
                    "company": job.get("company", "Confidential"),
                    "location": job.get("location", "Sri Lanka"),
                    "link": job.get("job_url", "#"),
                    "relevance_score": round(float(hit['score']) * 100, 1)
                })
                
            return results[:top_n]
        except Exception as e:
            if self.show_progress: print(f"Error matching jobs: {e}")
            return []

    
    # course classificatin
   
    # classify course level (updated for msc/degree)
    def classify_course_level(self, title, duration):
        title = str(title).lower()
        duration = str(duration).lower()

        # separate postgrad from undergrad
        if any(x in title for x in ["msc", "master", "phd", "doctorate", "postgraduate", "mba"]):
            return "Postgraduate"

        if any(x in title for x in ["degree", "bsc", "bachelor", "undergraduate"]):
            return "Academic (Degree)"
        
        if any(x in title for x in ["diploma", "hnd", "foundation"]):
            return "Academic (Diploma)"

        if any(x in title for x in ["advanced", "professional", "expert", "architect", "management"]):
            return "Professional"

        if any(x in title for x in ["intro", "basic", "beginner", "fundamental", "bootcamp"]):
            return "Beginner"

        return "Mid-Level"

    
    # get skills for job
    def get_skills_for_job(self, job_title):
        # try to find skills from local jobs first
        local_matches = self.jobs_df[
            self.jobs_df["title"].str.contains(job_title, case=False, na=False)
        ]
        
        local_skills = []
        if not local_matches.empty:
            # aggregate skills from top matches
            for idx, row in local_matches.head(10).iterrows():
                if pd.notna(row.get("extracted_skills")):
                    # assuming extracted_skills is a string
                    skills = row["extracted_skills"]
                    if isinstance(skills, str):
                        local_skills.extend([s.strip() for s in skills.split(",")])
                    elif isinstance(skills, list):
                        local_skills.extend(skills)
        
        local_skills = list(set(s for s in local_skills if len(s) > 2))
        
        # esco fallback using essential relations
        job_emb = self.model.encode(job_title, convert_to_tensor=True)
        hit = util.semantic_search(job_emb, self.esco_occ_embs, top_k=1)[0][0]
        occ = self.esco_occ.iloc[hit["corpus_id"]]

        # only pick essential skills to avoid random languages
        rel = self.occ_skill_rel[
            (self.occ_skill_rel["occupationUri"] == occ["conceptUri"]) & 
            (self.occ_skill_rel["relationType"] == "essential")
        ]
        esco_skills_all = self.esco_skills[
            self.esco_skills["conceptUri"].isin(rel["skillUri"])
        ]["preferredLabel"].tolist()

        #  Cross reference with available jobs
        if self.market_skills:
            esco_skills = [s for s in esco_skills_all if s.lower() in self.market_skills or any(ms in s.lower() for ms in self.market_skills)]
           
            if len(esco_skills) < 5:
                esco_skills = esco_skills_all
        else:
            esco_skills = esco_skills_all

        combined_skills = list(set(local_skills + esco_skills[:12]))
        
        return combined_skills, occ["preferredLabel"]
    
    
    def estimate_responsibility_band(self, user_skills, years_exp=0):
        """Estimate Responsibility Band (0-4) based on skills and years of experience"""
        band = 0
        
        # Base on years of experience
        if years_exp >= 10: band = 4
        elif years_exp >= 6: band = 3
        elif years_exp >= 3: band = 2
        elif years_exp >= 1: band = 1
        
        #  Skill-based adjustments (Look for "Complexity Signals")
        complexity_signals = {
            "strategy": 3, "leadership": 3, "architecture": 3, "management": 3,
            "budgeting": 3, "transformation": 4, "board": 4, "design patterns": 2,
            "refactoring": 2, "deployment": 1
        }
        
        for skill in user_skills:
            skill_lower = skill.lower()
            for signal, level in complexity_signals.items():
                if signal in skill_lower:
                    band = max(band, level)
                    
        return band

    def get_career_progression(self, current_role, current_band, user_skills, assessment_vector=None):
        """Generates progression paths using data-driven career maps and internship logic"""
        progression = []
        
        # Check for Internship/Entry Level Recommendation
        if assessment_vector and self._should_recommend_internships(assessment_vector):
            # Find internship paths matching interest or generic IT
            intern_paths = self.career_progressions_df[
                self.career_progressions_df['current_role'].str.contains('Intern', case=False, na=False)
            ]
            
            # Rank internships by relevance to User Skills & Intent
            scored_paths = []
            
            # Combine explicit skills and partial matches from role names
            search_terms = [s.lower() for s in user_skills]
            if current_role and current_role != "None":
                search_terms.append(current_role.lower())
                
            for _, row in intern_paths.iterrows():
                score = 0
                row_text = f"{row['track_name']} {row['next_role']} {row['requirements']}".lower()
                
                #  Keyword Matching (Skills & Interests)
                for term in search_terms:
                    if term in row_text:
                        score += 5  # Strong match
                    elif any(t in row_text for t in term.split()):
                        score += 1  # Partial match
                
                #  Track Name Priority (if current role matches track)
                if current_role and current_role.lower() in str(row['track_name']).lower():
                    score += 10
                    
                scored_paths.append((score, row))
            
            # Sort by score descending
            scored_paths.sort(key=lambda x: x[0], reverse=True)
            
            # Return top 3 filtered paths (or defaults if no match found)
            top_paths = [p[1] for p in scored_paths[:3]] if scored_paths and scored_paths[0][0] > 0 else intern_paths.head(3).iterrows()
            
            # Handle the iteration difference (list vs generator)
            iterable_paths = top_paths if isinstance(top_paths, list) else [r for _, r in top_paths]

            for row in iterable_paths:
                progression.append({
                    "type": "Entry Level (Internship)",
                    "role": row['next_role'], # The target is the Junior role
                    "current_step": row['current_role'],
                    "target_band": 1,
                    "typical_years": row['typical_years'],
                    "advice": f"Start here: {row['requirements'] or 'Gain foundational skills'}"
                })

        # Data-Driven Paths (Primary Method)
        found_data_driven = False
        if not self.career_progressions_df.empty:
            matches = self.career_progressions_df[
                self.career_progressions_df['current_role'].str.contains(current_role, case=False, na=False)
            ]
            
            if not matches.empty:
                found_data_driven = True
                for _, row in matches.head(2).iterrows():
                    progression.append({
                        "type": "Vertical (Promotion)",
                        "role": row['next_role'],
                        "target_band": current_band + 1,
                        "typical_years": row['typical_years'],
                        "advice": f"Progression path: {row['requirements']}"
                    })

        # 3. Fallback: Band-based Vertical Promotion (Legacy Logic)
        # Only add if we didn't find specific data-driven vertical paths
        if not found_data_driven and current_band < 4:
            target_band = current_band + 1
            band_labels = ["Intern", "Independent", "Professional", "Lead", "Executive"]
            
            curr_label = band_labels[current_band] if current_band < len(band_labels) else "Expert"
            target_label = band_labels[target_band] if target_band < len(band_labels) else "Leader"
            
            target_role = f"{target_label} {current_role}"
            
            progression.append({
                "type": "Vertical (Promotion)",
                "role": target_role,
                "target_band": target_band,
                "typical_years": "2-4 years",
                "advice": f"Focus on moving from {curr_label} to {target_label} responsibilities."
            })
            
        # 4. Horizontal Transition (Always suggest alternatives)
        # This adds variety (e.g. "DevOps" -> "SRE")
        alternates = self.suggest_alternate_paths(current_role)
        for alt in alternates[:2]:
            progression.append({
                "type": "Horizontal (Transition)",
                "role": alt,
                "target_band": current_band,
                "typical_years": "6-12 months",
                "advice": "Leverage your existing skill overlap to switch domains."
            })
            
        return progression

    def get_top_up_recommendations(self, current_band, target_band, segment):
        """Suggests specific course types (Top-ups) based on the band leap"""
        top_ups = []
        
        if current_band < 2 and target_band >= 2:
            top_ups.append("Professional Certifications (AWS, PMP, etc.)")
        if target_band >= 3:
            top_ups.append("Postgraduate Studies (MSc, MBA)")
        if segment == "Student" and current_band == 0:
            top_ups.append("Academic Degree (BSc/BEng)")
            
        return top_ups
        # Determine context-specific advice and top companies from dataset
        advice = f"Target {mapped_occ_name} roles in leading Sri Lankan organizations. Focus on professional certifications to advance."
        sector_name = "General"
        source_citation = "ESCO Hierarchy"

        if self.market_advice_df is not None:
            l_label = label.lower()
            for _, row in self.market_advice_df.iterrows():
                keywords = [k.strip().lower() for k in str(row['Keyword']).split(',')]
                if any(k in l_label for k in keywords):
                    sector_name = row['Sector']
                    advice = f"Target {sector_name} roles in {row['Companies']}. {row['Advice']}"
                    source_citation = row['Source']
                    break

        # Official Description from ESCO
        occ_desc_matches = self.esco_occ[self.esco_occ["conceptUri"] == uri]
        role_desc = occ_desc_matches["description"].iloc[0] if not occ_desc_matches.empty else ""

        progression.append({
            "target_role": sl_label,
            "estimated_years": years,
            "additional_info": advice,
            "role_description": role_desc,
            "source": f"ESCO Hierarchy / {source_citation}"
        })

        return progression
    def parse_resume(self, file_path):
        """Unified entry point for all resume types (PDF, JPG, PNG, AVIF)"""
        path = Path(file_path)
        ext = path.suffix.lower()
        
        if ext == ".pdf":
            return self.parse_resume_pdf(file_path)
        elif ext in [".jpg", ".jpeg", ".png", ".avif", ".webp"]:
            return self.parse_resume_image(file_path)
        else:
            if self.show_progress: print(f"Unsupported file format: {ext}")
            return []

    def parse_resume_image(self, image_path):
        """Extracts text from images using OCR (Tesseract)"""
        if not HAS_OCR:
            if self.show_progress: print("OCR libraries (PIL/pytesseract) not found. Cannot parse image.")
            return []
        
        try:
            img = Image.open(image_path)
            text = pytesseract.image_to_string(img)
            return self.parse_resume_text(text)
        except Exception as e:
            if self.show_progress: print(f"Error parsing image resume: {e}")
            return []

    def auto_profile(self, resume_path):
        """Suggests a target job based on skills extracted from a resume (PDF or Image)"""
        
        #  Extract skills (routes to PDF or OCR)
        skills = self.parse_resume(resume_path)
        if not skills:
            return {"extracted_skills": [], "suggested_target": "Unknown"}

        #  Convert skills into a "Profile Vector"
        # We join the skills into a single sentence so the model understands the context
        skill_text = "Experienced professional skilled in: " + ", ".join(skills)
        skill_emb = self.model.encode(skill_text, convert_to_tensor=True)
        
        #  Semantic Comparison
        # We compare your profile meaning against the pre-calculated meanings of 
        # every job title in our ESCO dataset (self.esco_occ_embs)
        hits = util.semantic_search(skill_emb, self.esco_occ_embs, top_k=1)[0]
        
        #  Return the Label
        # We get the index of the best match and pull its human-readable name
        best_match_idx = hits[0]["corpus_id"]
        suggested_job = self.esco_occ.iloc[best_match_idx]["preferredLabel"]
        
        return {
            "extracted_skills": skills,
            "suggested_target": suggested_job
        }

    def generate_skill_assessment_questions(self, skill_gap):
        """Generates interest and proficiency questions from the comprehensive assessment dataset"""
        questions = []
        
        #  Load comprehensive questions
        try:
            q_file = Path(__file__).parent.parent / "Machine Learning and Data Cleaning" / "data" / "raw" / "assessment" / "comprehensive_questions.json"
            if q_file.exists():
                with open(q_file, 'r') as f:
                    comp_qs = json.load(f)
                
                # Add 1 random universal question from each category
                import random
                for category, qs in comp_qs.items():
                    if qs:
                        q = random.choice(qs)
                        questions.append({
                            "type": "General",
                            "category": category,
                            "question": q["question"],
                            "options": q["options"]
                        })
        except Exception as e:
            print(f"Error loading assessment questions: {e}")

        #  Dynamic gap-specific questions
        for skill in skill_gap[:5]:
            is_tech = any(x in skill.lower() for x in ["programming", "software", "data", "tool", "system", "engine"])
            if is_tech:
                questions.append({
                    "type": "Technical",
                    "skill": skill,
                    "question": f"How would you rate your hands-on experience with {skill}?",
                    "options": ["None", "Beginner (Basic Syntax)", "Intermediate (Used in projects)", "Advanced (Expert)"]
                })
            else:
                questions.append({
                    "type": "Soft Skill",
                    "skill": skill,
                    "question": f"Are you familiar with the concepts of {skill}?",
                    "options": ["Not at all", "Somewhat familiar", "Very familiar", "Already proficient"]
                })
        return questions

    def parse_resume_text(self, resume_text):
        """Extracts skills from resume text using market index and semantic search"""
        found_skills = []
        text_lower = resume_text.lower()
        
        #. Direct Keyword Matching
        for skill in self.market_skills:
            if len(skill) > 3 and f" {skill} " in f" {text_lower} ":
                found_skills.append(skill.title())
        
        #. Semantic lookup for top skills mentioned
        # chunk resume text
        chunks = [resume_text[i:i+500] for i in range(0, len(resume_text), 500)]
        if chunks:
            # check first few chunks for job title/skills
            resume_emb = self.model.encode(chunks[0], convert_to_tensor=True)
            pass 

        return list(set(found_skills))[:15]

    def parse_resume_pdf(self, pdf_path):
        """Extracts text from PDF and matches skills"""
        try:
            import fitz
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            return self.parse_resume_text(text)
        except Exception as e:
            print(f"Error parsing PDF: {e}")
            return []

    def calculate_skill_score(self, resume_skills, assessment_data, target_skills):
        """Calculates a weighted skill score using adaptive signals"""
        if not target_skills: return 0.0
        
        #  Resume Signal (0.4)
        overlap = set(s.lower() for s in resume_skills) & set(s.lower() for s in target_skills)
        resume_score = len(overlap) / len(target_skills) if target_skills else 0
        
        #  Assessment Signal (0.4)
        assess_score = 0.5 # Default middle-ground
        if assessment_data:
            correct = sum(1 for q in assessment_data if q.get('is_correct'))
            assess_score = correct / len(assessment_data)
            
        #  Task Alignment (0.2)
        task_score = 1.0 # Default
        
        # Adaptive Weighting (Scenario B: No Resume)
        if not resume_skills:
            final_score = (assess_score * 0.75) + (task_score * 0.25)
        else:
            final_score = (resume_score * 0.4) + (assess_score * 0.4) + (task_score * 0.2)
            
        return round(final_score, 2)

    def calculate_readiness_score(self, user_skills, assessment_vector, target_role):
        """
        Calculates a 'Career Readiness Index' (0-100) based on 5 metrics:
        - Skills Match: 35%
        - Experience Level: 25%
        - Responsibility Level: 15%
        - Career Clarity: 15%
        - Communication/Behavior: 10%
        """
        all_required, _ = self.get_skills_for_job(target_role)
        
        #  Skills Match (35%)
        overlap = set(s.lower() for s in user_skills) & set(s.lower() for s in all_required)
        skill_pct = (len(overlap) / len(all_required)) if all_required else 0
        
        #  Experience Level (25%)
        exp_years = assessment_vector.get("experience_years", 0)
        # Normalize: 0=0, 5+=1.0
        exp_pct = min(exp_years / 5.0, 1.0)
        
        #  Responsibility Level (15%)
        band = self.estimate_responsibility_band(user_skills, exp_years)
        # Normalize: 0=0, 4=1.0
        resp_pct = band / 4.0
        
        #  Career Clarity (15%)
        # Based on status_level: 0 (Deciding) to 3 (Professional)
        status = assessment_vector.get("status_level", 0)
        clarity_pct = status / 3.0
        
        #  Communication/Behavior (10%)
        # Proxied by intent/open prompt completion
        comm_pct = 1.0 if assessment_vector.get("intent_embedding") else 0.5
        
        score = (skill_pct * 35) + (exp_pct * 25) + (resp_pct * 15) + (clarity_pct * 15) + (comm_pct * 10)
        
        breakdown = {
            "overall": round(score, 1),
            "skills_match": round(skill_pct * 100, 1),
            "experience": round(exp_pct * 100, 1),
            "responsibility": round(resp_pct * 100, 1),
            "clarity": round(clarity_pct * 100, 1),
            "communication": round(comm_pct * 100, 1),
            "stage": "Development Phase" if score < 70 else "Market Ready"
        }
        
        return breakdown

    def calculate_transferability_score(self, current_role, target_role):
        """Calculates skill overlap between roles for career switchers"""
        current_skills, _ = self.get_skills_for_job(current_role)
        target_skills, _ = self.get_skills_for_job(target_role)
        
        overlap = set(s.lower() for s in current_skills) & set(s.lower() for s in target_skills)
        missing = set(s.lower() for s in target_skills) - set(s.lower() for s in current_skills)
        
        diff = "Low"
        if len(missing) > 8: diff = "High"
        elif len(missing) > 4: diff = "Medium"
        
        return {
            "transferable_skills_count": len(overlap),
            "missing_core_skills_count": len(missing),
            "difficulty": diff,
            "estimated_time": "6-12 months" if diff == "Medium" else ("12+ months" if diff == "High" else "3-6 months")
        }

    def generate_action_plan(self, gap_skills, target_role="your target role"):
        """Generates a dynamic 12-month coaching roadmap based on skill gaps"""
        plan = []
        if not gap_skills:
            return [{"period": "Months 1-12", "focus": f"Maintenance: Already industry-ready for {target_role}. focus on networking."}]
        
        # Prioritize top 2 most critical skills
        core_focus = gap_skills[:2]
        remaining = gap_skills[2:5]
        
        # Group 1: Months 1-3
        plan.append({
            "period": "Months 1-3", 
            "focus": f"Foundations: Master {', '.join(core_focus)}",
            "milestone": "Complete initial technical baseline"
        })
        # Group 2: Months 4-6
        plan.append({
            "period": "Months 4-6", 
            "focus": f"Building: Portfolio projects using {core_focus[0]}",
            "milestone": "Github repository with 3+ projects"
        })
        # Group 3: Months 7-9
        plan.append({
            "period": "Months 7-9", 
            "focus": f"Visibility: Certification in {remaining[0] if remaining else 'Specialization'} and Internship hunt",
            "milestone": "Updated resume with newly acquired skills"
        })
        # Group 4: Months 10-12
        plan.append({
            "period": "Months 10-12", 
            "focus": f"Placement: Advanced {target_role} interview prep and Mentorship",
            "milestone": "Full-time role placement"
        })
        
        return plan

    def parse_resume_image(self, image_path):
        """OCR parsing for AVIF/PNG/JPG resumes"""
        try:
            from PIL import Image
            # Check for avif plugin
            try:
                import pillow_avif
            except ImportError:
                print("Warning: pillow-avif-plugin not found. AVIF support may be limited.")
            
            # Simple text extraction attempt (using fitz if it can handle images)
            import fitz
            doc = fitz.open(image_path)
            text = ""
            for page in doc:
                text += page.get_text()
            
            if len(text.strip()) < 50:
                print("Note: Image text is sparse. OCR recommended for real product.")
                # We'd plug in pytesseract here: 
                # text = pytesseract.image_to_string(Image.open(image_path))
                
            return self.parse_resume_text(text)
        except Exception as e:
            print(f"Error parsing image resume: {e}")
            return []

    def _extract_tasks_from_jd(self, jd_text):
        """Filters boilerplate and extracts core verbs/tools"""
        fluff = ["team player", "passionate", "communication", "hardworking", "responsible"]
        sentences = re.split(r'[.!?\r\n]', jd_text)
        tasks = []
        for s in sentences:
            s_clean = s.lower().strip()
            if len(s_clean) < 10 or any(f in s_clean for f in fluff):
                continue
            if any(skill.lower() in s_clean for skill in self.market_skills):
                tasks.append(s.strip())
        return list(set(tasks))[:5]

    def _estimate_market_average(self, level, category, provider=None):
        """Estimates fees based on Provider first, then Level using loaded config"""
        
        #  Specific Provider Estimates
        if provider and "provider_estimates" in self.pricing_config:
            for key, val in self.pricing_config["provider_estimates"].items():
                if key.lower() in provider.lower():
                    return {"duration": "Varies", "fee": f"~{val} (Est)"}

        #  Level-based Fallbacks
        defaults = {"duration": "Contact Provider", "fee": "Contact Provider"}
        if "level_averages" in self.pricing_config:
            return self.pricing_config["level_averages"].get(level, defaults)
            
        return defaults

    # recommend courses

    def _process_one_course(self, course, similarity_score, segment, user_level, location, max_budget, max_duration, skill_gap):
        """Helper to score and format a single course/degree"""
        level = self.classify_course_level(str(course["course_title"]), str(course.get("duration", "N/A")))
        score = similarity_score

        #  Level-based Scoring
        if segment == "Student":
            if level == "Professional": score *= 0.5
            elif level == "Postgraduate": score *= 0.2
            elif level == "Academic (Degree)": score *= 1.8
            elif level == "Academic (Diploma)": score *= 1.4
        elif segment == "Professional":
            if user_level in ["Mid", "Senior", "Lead", "Manager", "Executive"]:
                if level == "Academic (Degree)": score *= 0.15
                elif level == "Postgraduate": score *= 1.8
                elif level == "Professional": score *= 1.6
            else:
                if level == "Academic (Degree)": score *= 0.8
                elif level == "Professional": score *= 1.8

        # Location Boost
        if location and pd.notna(course.get("location")):
            if str(location).lower() in str(course["location"]).lower():
                score *= 1.3

        #  Fee & Duration Penalties
        # Use numeric if available, else estimate
        current_fee = course.get("fee_numeric", 0) if "fee_numeric" in course else 0
        if current_fee == 0 and "cost" in course and pd.notna(course["cost"]):
            nums = re.findall(r'\d+', str(course["cost"]).replace(',', ''))
            if nums: current_fee = int(nums[0])
            
        if max_budget and current_fee > max_budget:
            score *= 0.1
        
        current_duration = course.get("duration_numeric", 0) if "duration_numeric" in course else 0
        if max_duration and current_duration > max_duration:
            score *= 0.1

        #  Fill in missing presentation data
        market_data = self._estimate_market_average(level, course.get("category", "General"), course.get("provider", ""))
        duration = course.get("duration") if pd.notna(course.get("duration")) and str(course.get("duration")) != "nan" else market_data["duration"]
        fee = course.get("cost") if "cost" in course and pd.notna(course["cost"]) else f"{int(current_fee):,} LKR" if current_fee > 0 else market_data["fee"]

        #  Rationale (Why Recommended)
        why = []
        if segment == "Student" and level == "Academic (Degree)": why.append("Core academic foundation")
        if location and location.lower() in str(course.get("location", "")).lower(): why.append(f"Located in {location}")
        for skill in skill_gap[:2]:
            if skill.lower() in str(course["course_title"]).lower():
                why.append(f"Teaches {skill}")

        return {
            "course_name": course["course_title"],
            "provider": course.get("provider", "Unknown Institution"),
            "level": level,
            "type": course.get("type", "Unknown"),
            "duration": duration,
            "fee": fee,
            "fee_numeric": current_fee,
            "location": course.get("location", "Online/Distance"),
            "relevance_score": round(score, 3),
            "why": why[:3]
        }

    def recommend_courses(
        self,
        user_skills,
        target_job,
        user_level="Entry",
        segment="Student",
        preference=None,
        location=None,
        max_budget=None,
        max_duration=None,
        top_n=5,
        assessment_vector=None
    ):
        # get skills and wanted role
        all_required, mapped_occ = self.get_skills_for_job(target_job)
        
        # find which are essential vs optional
        occ_uri = self.esco_occ[self.esco_occ["preferredLabel"] == mapped_occ].iloc[0]["conceptUri"]
        essential_uris = self.occ_skill_rel[
            (self.occ_skill_rel["occupationUri"] == occ_uri) & 
            (self.occ_skill_rel["relationType"] == "essential")
        ]["skillUri"].tolist()
        
        essential_skills = set(self.esco_skills[self.esco_skills["conceptUri"].isin(essential_uris)]["preferredLabel"].str.lower().tolist())

        user_skill_set = set(s.lower() for s in user_skills)
        
        # split gaps into compulsory vs optional
        compulsory_gap = [s for s in all_required if s.lower() in essential_skills and s.lower() not in user_skill_set]
        optional_gap = [s for s in all_required if s.lower() not in essential_skills and s.lower() not in user_skill_set]
        
        skill_gap = compulsory_gap + optional_gap

        # 2. Calculate Signals (Bands & Multi-Signal Score)
        band = self.estimate_responsibility_band(user_skills)
        current_score_val = self.calculate_skill_score(user_skills, None, all_required)
        
        # 3. Handle Complete Match
        if not skill_gap:
            return {
                "status": "Complete",
                "message": "No skill gap detected for this role.",
                "band": band,
                "score": current_score_val
            }

        #  Build Query for Courses
        # Cleaner Query: Filter out verbose ESCO skill names (often full sentences)
        # This prevents the "cluttering" of the query
        query_skills = []
        #  ESCO Mapping & Gap Analysis
        compulsory_skills, mapped_occ = self.get_skills_for_job(target_job)
        optional_skills = [] # Fallback for now

        user_skills_lower = [s.lower() for s in user_skills]
        compulsory_gap = [s for s in compulsory_skills if s.lower() not in user_skills_lower]
        optional_gap = []
        skill_gap = compulsory_gap

        # Career Banding
        band = self.estimate_responsibility_band(user_skills)
        print(f"DEBUG: Career band = {band}")

        #  Semantic Query Construction
        query_skills = []
        for s in compulsory_gap[:4]:
            if len(s) > 40: query_skills.extend(s.split()[:3])
            else: query_skills.append(s)
        
        query_terms = query_skills + optional_gap[:2]
        if segment == "Student": query_terms += ["degree", "bachelor", "bsc", "university"]
        if preference: query_terms.append(preference)
        query = " ".join(query_terms)
        print(f"DEBUG: Query = {query}")
        
        query_emb = self.model.encode(query, convert_to_tensor=True)
        print(f"DEBUG: query_emb type = {type(query_emb)}")

        #  SEMANTIC SEARCH - Professional Courses
        print(f"DEBUG: self.course_embs type = {type(self.course_embs)}")
        hits = util.semantic_search(query_emb, self.course_embs, top_k=top_n * 20)[0]
        print(f"DEBUG: hits[0] type = {type(hits[0]) if hits else 'Empty'}")
        recommendations = []
        for h in hits:
            course = self.courses_df.iloc[h["corpus_id"]]
            processed = self._process_one_course(course, h["score"], segment, user_level, location, max_budget, max_duration, skill_gap)
            recommendations.append(processed)

        # SEMANTIC SEARCH - Academic (Degree) Programs (Supplementary)
        academic_recommendations = []
        if self.academic_embs is not None:
            acad_hits = util.semantic_search(query_emb, self.academic_embs, top_k=top_n * 10)[0]
            for h in acad_hits:
                course = self.academic_df.iloc[h["corpus_id"]]
                processed = self._process_one_course(course, h["score"], segment, user_level, location, max_budget, max_duration, skill_gap)
                # Apply Supplementary penalty
                processed["relevance_score"] *= 0.6 
                academic_recommendations.append(processed)

        # Sort and limit
        recommendations = sorted(recommendations, key=lambda x: x['relevance_score'], reverse=True)[:top_n]
        academic_recommendations = sorted(academic_recommendations, key=lambda x: x['relevance_score'], reverse=True)[:top_n]

        #  JOB FETCHING
        job_list = []
        try:
            job_hits = util.semantic_search(query_emb, self.job_embs, top_k=5)[0]
            for h in job_hits:
                j = self.jobs_df.iloc[h["corpus_id"]]
                j_raw_skills = str(j.get("extracted_skills", "")).lower()
                job_overlap = [s for s in compulsory_skills if s.lower() in j_raw_skills]
                gap_pct = 100 - round(100 * (len(job_overlap) / max(1, len(compulsory_skills))), 1)
                
                job_list.append({
                    "job_title": j["title"],
                    "company": j.get("company", "Lankan Employer"),
                    "deadline": j.get("deadline", "Apply Soon"),
                    "url": j.get("url", j.get("job_url", "#")),
                    "skill_gap_pct": gap_pct,
                    "relevance_score": round(h["score"], 3),
                    "estimated_salary": self.get_salary_for_role(j["title"])
                })
            
            if not job_list:
                job_list.append({
                    "job_title": "No specific openings found",
                    "company": "Market Research Advised",
                    "message": "We couldn't find active jobs for this specific search. Try broadening your location or role."
                })
        except Exception as e:
            print(f"INFO: Job fetching skipped: {e}")
            job_list.append({"job_title": "Job Service Busy", "message": str(e)})

        #  ADVICE BOX
        advice = []
        all_res = recommendations + academic_recommendations
        valid_fees = [r['fee_numeric'] for r in all_res if r['fee_numeric'] > 0]
        if max_budget and valid_fees and min(valid_fees) > max_budget:
            advice.append("Note: Most programs exceed your budget. Target State Universities or OUSL.")
        if location and all_res and not any(location.lower() in str(r['location']).lower() for r in all_res):
             advice.append(f"No direct matches in {location.title()}, showing Online options.")

        #  FINAL BUNDLE
        return {
            "status": "Incomplete" if skill_gap else "Complete",
            "mapped_occupation": mapped_occ,
            "compulsory_skills": compulsory_gap[:8],
            "optional_skills": optional_gap[:8],
            "assessment_questions": self.generate_skill_assessment_questions(compulsory_gap + optional_gap),
            "recommendations": recommendations,
            "academic_recommendations": academic_recommendations,
            "job_ideas": job_list if job_list else [{"job_title": "No specific openings found for this niche", "company": "Market Research Advised", "message": "Try broadening your role or location filters."}],
            "mentors": self.match_mentors(user_skills, top_n=3) if user_skills else [],
            "alternate_paths": self.suggest_alternate_paths(target_job),
            "career_progression": self.get_career_progression(target_job, band, user_skills, assessment_vector),
            "salary_estimate": self.get_salary_for_role(target_job),
            "readiness_score": self.calculate_readiness_score(user_skills, assessment_vector or {"status_level": 1 if segment=="Student" else 2, "experience_years": 0}, target_job),
            "action_plan": self.generate_action_plan(compulsory_gap, target_role=target_job),
            "market_trends": self.get_personalized_market_trends(target_job),
            "caveats": advice
        }

        return paths

    def suggest_alternate_paths(self, job_title):
        # simplified version using esco similarity
        job_emb = self.model.encode(job_title, convert_to_tensor=True)
        hits = util.semantic_search(job_emb, self.esco_occ_embs, top_k=4)[0]
        
        paths = []
        for h in hits[1:]: # skip the first one as it's the target itself
            alt_job = self.esco_occ.iloc[h["corpus_id"]]["preferredLabel"]
            paths.append(alt_job)
        return paths

    def match_mentors(self, user_skills, top_n=3):
        """
        Finds mentors whose expertise overlaps with user skills.
        """
        if not self.mentors_data:
            return []

        scored_mentors = []
        user_skill_set = set(s.lower() for s in user_skills)

        for mentor in self.mentors_data:
            # Score based on skill overlap
            mentor_skills = []
            raw_skills = mentor.get('skills')
            
            if isinstance(raw_skills, list):
                mentor_skills = [str(s).lower() for s in raw_skills]
            elif isinstance(raw_skills, str):
                mentor_skills = [s.strip().lower() for s in raw_skills.split(',')]
            
            # Fallback to expertise/bio if skills are missing
            if not mentor_skills:
                 bio = str(mentor.get('bio', '')).lower()
                 expertise = str(mentor.get('expertise', '')).lower() 
                 combined = bio + " " + expertise
                 # Check if user skills appear in bio
                 match_in_bio = [s for s in user_skill_set if s in combined]
                 overlap = match_in_bio
            else:
                overlap = [s for s in mentor_skills if s in user_skill_set]
            
            score = len(overlap) * 2
            
            # Boost by specialization match
            specialization = str(mentor.get('specialization', '')).lower()
            if any(s in specialization for s in user_skill_set):
                score += 3
            
            # 
            if "senior" in str(mentor.get('title')).lower():
                score += 1

            # if premium
            is_premium = mentor.get('tier') == 'Premium' or mentor.get('is_premium', False)

            scored_mentors.append({
                "name": mentor.get('name'),
                "title": mentor.get('current_role', mentor.get('title', 'Mentor')),
                "company": mentor.get('company'),
                "specialization": mentor.get('specialization'),
                "score": score,
                "is_premium": is_premium,
                "matched_skills": overlap
            })

        # Sort by score descending
        scored_mentors.sort(key=lambda x: x['score'], reverse=True)
        
        return scored_mentors[:top_n]

    def suggest_mentor(self, sector_or_job):
        """Mental Suggestion using loaded synthetic data"""
        if not hasattr(self, 'mentors_data') or not self.mentors_data:
            return ["PathFinder+ Alumni", "Industry Professional"]
        
        # Filter by sector or keyword
        relevant = [
            f"{m['name']} ({m['title']} at {m['company']})" 
            for m in self.mentors_data 
            if sector_or_job.lower() in m['sector'].lower() or sector_or_job.lower() in m['title'].lower()
        ]
        
        if relevant:
            return relevant[:3] # Return top 3 matches
        
        # If no direct match, return randoms from same sector if possible, or just randoms
        return [f"{m['name']} ({m['title']} at {m['company']})" for m in self.mentors_data[:3]]

    def suggest_career_direction(self, interests: List[str]):
        """Interest-to-Career mapping for early students (O/L, A/L)"""
        mapping = {
            "math": "Engineering / Data Science",
            "art": "UI/UX Design / Creative Marketing",
            "business": "Management / Accounting / Logistics",
            "science": "Bio-Tech / Medicine / Engineering",
            "logic": "Software Engineering / Legal",
            "drawing": "Architecture / Graphic Design"
        }
        
        discovered = []
        for interest in interests:
            interest_lower = interest.lower()
            for key in mapping:
                if key in interest_lower:
                    discovered.append(mapping[key])
        
        return list(set(discovered)) if discovered else ["General Management / Social Sciences"]

    def get_personalized_market_trends(self, target_role):
        """Detects user field and fetches relevant market trends"""
        # Simple field detection logic
        field = "General"
        it_keywords = ["software", "developer", "data", "it", "web", "cloud", "engineer", "network", "ai", "tech"]
        biz_keywords = ["manager", "business", "analyst", "finance", "accounting", "hr", "sales", "operations"]
        marketing_keywords = ["marketing", "social media", "content", "brand", "seo"]
        
        role_lower = str(target_role).lower()
        if any(k in role_lower for k in it_keywords): field = "IT"
        elif any(k in role_lower for k in biz_keywords): field = "Business"
        elif any(k in role_lower for k in marketing_keywords): field = "Marketing"
        
        # Check cache
        if field in self._trend_cache:
            return self._trend_cache[field]
            
        try:
            trends, field_df = self.trend_analyzer.get_trends_by_field(field)
            hot_skills = self.trend_analyzer.get_hot_skills(5, df=field_df)
            
            result = {
                "field": field,
                "segments": trends,
                "top_demanded_skills": hot_skills
            }
            self._trend_cache[field] = result
            return result
        except Exception as e:
            return {"error": f"Could not load trends: {str(e)}", "field": field}



if __name__ == "__main__":
    current_dir = Path(__file__).parent
    engine = RecommendationEngine(
        jobs_path=current_dir / "../data/processed/all_jobs_master.csv",
        courses_path=current_dir / "../data/processed/all_courses_master.csv",
        esco_dir=current_dir / "../data/raw/esco",
        force_refresh=False
    )


    # print("\nTEST 1: A/L Student -> Degree")
    # try:
    #     res = engine.recommend_courses(
    #         ["Math"], "Software Engineer", segment="Student", preference="Degree"
    #     )
    # except Exception:
    #     print(traceback.format_exc())
    #     sys.exit(1)
    
    # print(f"Mapped Occupation: {res['mapped_occupation']}")
    # print(f"Career Progression: {res['career_progression']}")
    # print(f"Salary Estimate: {res['salary_estimate']}")
    
    # print("\n--- PROFESSIONAL RECOMMENDATIONS ---")
    # for r in res["recommendations"]:
    #     print(f"- {r['course_name']} | {r['provider']} ({r['level']}) [Score: {r['relevance_score']}]")
        
    # print("\n--- ACADEMIC RECOMMENDATIONS ---")
    # for r in res["academic_recommendations"]:
    #     print(f"- {r['course_name']} | {r['provider']} ({r['level']}) [Score: {r['relevance_score']}]")

    # print("\n--- RECOMMENDED MENTORS ---")
    # for m in res["mentors"]:
    #     print(f"- {m['name']} ({m['title']} at {m['company']}) [Match: {m['score']}]")
