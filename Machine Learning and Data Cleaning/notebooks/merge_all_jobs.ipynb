{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Merge All Job Data Sources\n",
                "\n",
                "This notebook merges job data from multiple scrapers (XpressJobs, LinkedIn, TopJobs, Ikman) into a single master dataset: `data/processed/all_jobs_master.csv`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Reading from: C:\\Users\\User\\Desktop\\Second Year Stuff\\sdgp\\PathFinder+\\Adjusted_Scraper\\project_root\\data\\raw\\jobs\n",
                        "Saving to: C:\\Users\\User\\Desktop\\Second Year Stuff\\sdgp\\PathFinder+\\Adjusted_Scraper\\project_root\\data\\processed\n"
                    ]
                }
            ],
            "source": [
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "import glob\n",
                "import os\n",
                "\n",
                "\n",
                "PROJECT_ROOT = Path(\"..\").resolve()\n",
                "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\" / \"jobs\"\n",
                "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
                "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "print(f\"Reading from: {RAW_DIR}\")\n",
                "print(f\"Saving to: {PROCESSED_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "def standardize_columns(df, source_name):\n",
                "    df = df.copy()\n",
                "    \n",
                "    # Define mappings based on source\n",
                "\n",
                "    if source_name == 'XpressJobs':\n",
                "        # Expected: title, company, location, description, job_url, scraped_date\n",
                "        mapping = {\n",
                "            'job_url': 'url',\n",
                "            'scraped_date': 'date'\n",
                "        }\n",
                "        data_type = 'Live'\n",
                "    elif source_name == 'LinkedIn':\n",
                "        # Expected: title, company_name, location, description, job_url, posted_date\n",
                "        mapping = {\n",
                "            'title': 'title',\n",
                "            'company_name': 'company',\n",
                "            'job_url': 'url',\n",
                "            'posted_date': 'date'\n",
                "        }\n",
                "        data_type = 'Training' # Per user request: LinkedIn data is for training models, not live application\n",
                "    elif source_name == 'TopJobs':\n",
                "        # Expected: title, company, description, date? job_url?\n",
                "        # TopJobs might vary, check your specific file\n",
                "        mapping = {\n",
                "            'opening_date': 'date',\n",
                "            'url': 'url'\n",
                "        }\n",
                "        data_type = 'Live'\n",
                "    elif source_name == 'Ikman':\n",
                "        # Expected: job_title, company, description, etc.\n",
                "        mapping = {\n",
                "            'job_title': 'title',\n",
                "            'company_name': 'company',\n",
                "            'job_url': 'url',\n",
                "            'posted_date': 'date'\n",
                "        }\n",
                "        data_type = 'Live'\n",
                "    else:\n",
                "        mapping = {}\n",
                "        data_type = 'Unknown'\n",
                "\n",
                "    # Renaissance logic: Rename available columns\n",
                "    df.rename(columns=mapping, inplace=True)\n",
                "    \n",
                "    # Ensure required columns exist\n",
                "    required = ['title', 'company', 'location', 'description', 'date', 'url']\n",
                "    for col in required:\n",
                "        if col not in df.columns:\n",
                "            df[col] = None  # Fill missing with None\n",
                "            \n",
                "    # Add source and type columns\n",
                "    df['source'] = source_name\n",
                "    df['data_type'] = data_type\n",
                "    \n",
                "    # Return only the standard columns\n",
                "    return df[required + ['source', 'data_type']]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "all_dataframes = []\n",
                "\n",
                "# -- XpressJobs --\n",
                "xpress_files = glob.glob(str(RAW_DIR / \"xpressjobs_ALL_CATEGORIES_CLEAN_*.csv\"))\n",
                "for f in xpress_files:\n",
                "    try:\n",
                "        df = pd.read_csv(f)\n",
                "        df = standardize_columns(df, 'XpressJobs')\n",
                "        all_dataframes.append(df)\n",
                "        print(f\"Loaded XpressJobs: {len(df)} rows\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed XpressJobs {f}: {e}\")\n",
                "\n",
                "# -- LinkedIn --\n",
                "linkedin_files = glob.glob(str(RAW_DIR / \"linkedin_sri_lanka_IT_jobs.csv\"))\n",
                "for f in linkedin_files:\n",
                "    try:\n",
                "        df = pd.read_csv(f)\n",
                "        df = standardize_columns(df, 'LinkedIn')\n",
                "        all_dataframes.append(df)\n",
                "        print(f\"Loaded LinkedIn: {len(df)} rows\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed LinkedIn {f}: {e}\")\n",
                "\n",
                "# -- TopJobs --\n",
                "topjobs_files = glob.glob(str(RAW_DIR / \"topjobs_FINAL_*.csv\"))\n",
                "for f in topjobs_files:\n",
                "    try:\n",
                "        df = pd.read_csv(f)\n",
                "        df = standardize_columns(df, 'TopJobs')\n",
                "        all_dataframes.append(df)\n",
                "        print(f\"Loaded TopJobs: {len(df)} rows\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed TopJobs {f}: {e}\")\n",
                "        \n",
                "# -- Ikman --\n",
                "ikman_files = glob.glob(str(RAW_DIR / \"ikman_*.csv\"))\n",
                "for f in ikman_files:\n",
                "    try:\n",
                "        df = pd.read_csv(f)\n",
                "        df = standardize_columns(df, 'Ikman')\n",
                "        all_dataframes.append(df)\n",
                "        print(f\"Loaded Ikman: {len(df)} rows\")\n",
                "    except Exception as e:\n",
                "        print(f\"Failed Ikman {f}: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "No data loaded!\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "if all_dataframes:\n",
                "    master_df = pd.concat(all_dataframes, ignore_index=True)\n",
                "    print(f\"\\nTotal rows loaded: {len(master_df)}\")\n",
                "    \n",
                "    # Deduplicate by URL (if exists) or Title+Company\n",
                "    # Some legacy scraped/manual data might not have URLs\n",
                "    \n",
                "    # Strategy: \n",
                "    # 1. Drop exact duplicates\n",
                "    master_df.drop_duplicates(inplace=True)\n",
                "    \n",
                "    # 2. Drop rows with NO title\n",
                "    master_df = master_df.dropna(subset=['title'])\n",
                "    \n",
                "    # 3. Drop duplicates on URL where URL is not null\n",
                "    # We separate rows with URL and rows without\n",
                "    with_url = master_df[master_df['url'].notna()]\n",
                "    no_url = master_df[master_df['url'].isna()]\n",
                "    \n",
                "    with_url = with_url.drop_duplicates(subset=['url'], keep='last')\n",
                "    \n",
                "    # Re-combine\n",
                "    final_df = pd.concat([with_url, no_url], ignore_index=True)\n",
                "    \n",
                "    print(f\"Final unique count: {len(final_df)}\")\n",
                "    \n",
                "    # Save\n",
                "    output_path = PROCESSED_DIR / \"all_jobs_master.csv\"\n",
                "    final_df.to_csv(output_path, index=False)\n",
                "    print(f\"Saved to {output_path}\")\n",
                "    \n",
                "    # Show Distribution\n",
                "    print(\"\\n--- Combined Data Distribution ---\")\n",
                "    print(final_df['data_type'].value_counts())\n",
                "    print(\"\\n\")\n",
                "    print(final_df['source'].value_counts())\n",
                "else:\n",
                "    print(\"No data loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "if 'final_df' in locals():\n",
                "    display(final_df.sample(5))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
